gmaps-scraper-python/
├── venv/                    # Virtual environment
├── config/
│   ├── __init__.py         ✅
│   ├── settings.py         ✅
│   └── selectors.py        ✅
├── src/
│   ├── __init__.py         ✅
│   ├── models/
│   │   ├── __init__.py     ✅
│   │   └── business.py     ✅
│   └── utils/
│       ├── __init__.py     ✅
│       ├── logger.py       ✅
│       ├── file_handler.py ✅
│       └── rate_limiter.py ✅
├── data/
│   ├── output/
│   │   ├── csv/            ✅
│   │   ├── json/           ✅
│   │   └── excel/          ✅
│   └── logs/               ✅
├── requirements.txt        ✅
├── .env                    ✅
├── .env.example           ✅
├── main.py                ✅
└── setup.py               ✅
























# Google Maps Scraper - Complete Project Structure

## Option 1: Python + Selenium Version

```
gmaps-scraper-python/
├── README.md
├── requirements.txt
├── .env.example
├── .gitignore
├── config/
│   ├── __init__.py
│   ├── settings.py
│   └── selectors.py
├── src/
│   ├── __init__.py
│   ├── scraper/
│   │   ├── __init__.py
│   │   ├── gmaps_scraper.py
│   │   ├── browser_manager.py
│   │   └── data_extractor.py
│   ├── utils/
│   │   ├── __init__.py
│   │   ├── logger.py
│   │   ├── proxy_rotator.py
│   │   ├── rate_limiter.py
│   │   └── file_handler.py
│   └── models/
│       ├── __init__.py
│       └── business.py
├── data/
│   ├── output/
│   │   ├── csv/
│   │   └── json/
│   ├── logs/
│   └── proxies/
│       └── proxy_list.txt
├── tests/
│   ├── __init__.py
│   ├── test_scraper.py
│   └── test_utils.py
└── main.py
```

### Key Files Content Structure:

#### `requirements.txt`
```
selenium==4.15.0
webdriver-manager==4.0.1
pandas==2.1.0
python-dotenv==1.0.0
requests==2.31.0
fake-useragent==1.4.0
beautifulsoup4==4.12.2
lxml==4.9.3
openpyxl==3.1.2
```

#### `config/settings.py`
```python
import os
from dotenv import load_dotenv

load_dotenv()

# Browser Settings
HEADLESS_MODE = os.getenv('HEADLESS_MODE', 'True').lower() == 'true'
BROWSER_TIMEOUT = int(os.getenv('BROWSER_TIMEOUT', '30'))
PAGE_LOAD_TIMEOUT = int(os.getenv('PAGE_LOAD_TIMEOUT', '20'))

# Scraping Settings
SCROLL_PAUSE_TIME = float(os.getenv('SCROLL_PAUSE_TIME', '2'))
CLICK_DELAY = float(os.getenv('CLICK_DELAY', '1.5'))
MAX_BUSINESSES_PER_SEARCH = int(os.getenv('MAX_BUSINESSES', '100'))

# Rate Limiting
MIN_DELAY_BETWEEN_REQUESTS = float(os.getenv('MIN_DELAY', '1'))
MAX_DELAY_BETWEEN_REQUESTS = float(os.getenv('MAX_DELAY', '3'))

# Output Settings
OUTPUT_FORMAT = os.getenv('OUTPUT_FORMAT', 'csv')  # csv, json, excel
OUTPUT_DIR = os.getenv('OUTPUT_DIR', 'data/output')

# Proxy Settings
USE_PROXY = os.getenv('USE_PROXY', 'False').lower() == 'true'
PROXY_LIST_FILE = os.getenv('PROXY_LIST_FILE', 'data/proxies/proxy_list.txt')
```

#### `src/models/business.py`
```python
from dataclasses import dataclass
from typing import Optional, List

@dataclass
class Business:
    name: str
    address: Optional[str] = None
    phone: Optional[str] = None
    website: Optional[str] = None
    rating: Optional[float] = None
    reviews_count: Optional[int] = None
    category: Optional[str] = None
    hours: Optional[str] = None
    price_range: Optional[str] = None
    coordinates: Optional[tuple] = None
    google_maps_url: Optional[str] = None
    
    def to_dict(self) -> dict:
        return {
            'name': self.name,
            'address': self.address,
            'phone': self.phone,
            'website': self.website,
            'rating': self.rating,
            'reviews_count': self.reviews_count,
            'category': self.category,
            'hours': self.hours,
            'price_range': self.price_range,
            'latitude': self.coordinates[0] if self.coordinates else None,
            'longitude': self.coordinates[1] if self.coordinates else None,
            'google_maps_url': self.google_maps_url
        }
```

---

## Option 2: Node.js + Puppeteer Version

```
gmaps-scraper-nodejs/
├── package.json
├── package-lock.json
├── README.md
├── .env.example
├── .gitignore
├── config/
│   ├── settings.js
│   └── selectors.js
├── src/
│   ├── scraper/
│   │   ├── GMapsScraper.js
│   │   ├── BrowserManager.js
│   │   └── DataExtractor.js
│   ├── utils/
│   │   ├── logger.js
│   │   ├── proxyRotator.js
│   │   ├── rateLimiter.js
│   │   └── fileHandler.js
│   └── models/
│       └── Business.js
├── data/
│   ├── output/
│   │   ├── csv/
│   │   └── json/
│   ├── logs/
│   └── proxies/
│       └── proxy-list.txt
├── tests/
│   ├── scraper.test.js
│   └── utils.test.js
└── index.js
```

#### `package.json`
```json
{
  "name": "gmaps-scraper",
  "version": "1.0.0",
  "description": "Google Maps business data scraper using Puppeteer",
  "main": "index.js",
  "scripts": {
    "start": "node index.js",
    "dev": "nodemon index.js",
    "test": "jest"
  },
  "dependencies": {
    "puppeteer": "^21.0.0",
    "puppeteer-extra": "^3.3.6",
    "puppeteer-extra-plugin-stealth": "^2.11.2",
    "csv-writer": "^1.6.0",
    "dotenv": "^16.3.1",
    "winston": "^3.10.0",
    "axios": "^1.5.0",
    "cheerio": "^1.0.0-rc.12",
    "user-agents": "^1.0.1456"
  },
  "devDependencies": {
    "nodemon": "^3.0.1",
    "jest": "^29.6.4"
  }
}
```

---

## Common Project Files

### `.env.example`
```env
# Browser Configuration
HEADLESS_MODE=true
BROWSER_TIMEOUT=30000
PAGE_LOAD_TIMEOUT=20000

# Scraping Settings
SCROLL_PAUSE_TIME=2
CLICK_DELAY=1.5
MAX_BUSINESSES=100

# Rate Limiting
MIN_DELAY=1
MAX_DELAY=3

# Output Settings
OUTPUT_FORMAT=csv
OUTPUT_DIR=./data/output

# Proxy Settings
USE_PROXY=false
PROXY_LIST_FILE=./data/proxies/proxy-list.txt

# Google Sheets (Optional)
GOOGLE_SHEETS_ENABLED=false
GOOGLE_SHEETS_ID=your_sheet_id
GOOGLE_SERVICE_ACCOUNT_KEY=path/to/service-account.json
```

### `.gitignore`
```
# Environment files
.env
.env.local

# Dependencies
node_modules/
__pycache__/
*.pyc
venv/
env/

# Output files
data/output/*.csv
data/output/*.json
data/output/*.xlsx
data/logs/*.log

# Browser data
.browser_data/
screenshots/

# IDE files
.vscode/
.idea/
*.swp
*.swo

# OS files
.DS_Store
Thumbs.db

# Proxy lists (keep template only)
data/proxies/proxy-list.txt
```

### `README.md` Structure
```markdown
# Google Maps Scraper

## Features
- Extract business data from Google Maps
- Support for multiple output formats (CSV, JSON, Excel)
- Proxy rotation support
- Rate limiting and anti-detection measures
- Configurable search parameters
- Logging and error handling

## Installation
[Installation steps]

## Usage
[Usage examples]

## Configuration
[Configuration options]

## Legal Disclaimer
[Important legal information about web scraping]
```

---

## Core Components Breakdown

### 1. **Main Scraper Classes**
- **GMapsScraper**: Main orchestrator class
- **BrowserManager**: Handles browser initialization, proxy setup
- **DataExtractor**: Extracts data from individual business pages

### 2. **Utility Classes**
- **Logger**: Structured logging for debugging and monitoring
- **ProxyRotator**: Manages proxy rotation to avoid IP blocks
- **RateLimiter**: Controls request frequency
- **FileHandler**: Manages output file operations

### 3. **Data Flow**
```
Search Query → Load Results → Scroll & Load More → 
Extract Business Links → Visit Each Business → 
Extract Details → Save to File → Continue
```

### 4. **Anti-Detection Features**
- Random user agents
- Proxy rotation
- Variable delays between actions
- Stealth mode (Puppeteer)
- Human-like scrolling patterns

### 5. **Output Formats**
- CSV files with all business data
- JSON for programmatic use
- Excel with formatted sheets
- Optional Google Sheets integration

### 6. **Error Handling**
- Retry mechanisms for failed requests
- Graceful handling of missing elements
- Comprehensive logging
- Resume functionality for interrupted scrapes

## Getting Started Commands

### Python Version:
```bash
git clone <repo>
cd gmaps-scraper-python
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env
python main.py
```

### Node.js Version:
```bash
git clone <repo>
cd gmaps-scraper-nodejs
npm install
cp .env.example .env
npm start
```

This structure provides a solid foundation for building a robust Google Maps scraper with proper organization, error handling, and scalability considerations.